{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark= (SparkSession.builder.appName(\"Capstone\")\\\n",
    "       .config(\"hive.metastore.uris\",\"thrift://ip-10-1-2-24.ap-south-1.compute.internal:9083\")\n",
    "       .enableHiveSupport().getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-1-1-204.ap-south-1.compute.internal:43002\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Capstone</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb40036ae10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "# from pyspark.sql.functions import * #lot of functions available here\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import when, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A list showing employee number, last name, first name, sex, and salary for each employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+---+------+\n",
      "|emp_no|last_name|first_name|sex|salary|\n",
      "+------+---------+----------+---+------+\n",
      "|473302|Zallocco |Hideyuki  |M  |40000 |\n",
      "|475053|Delgrande|Byong     |F  |53422 |\n",
      "|57444 |Babb     |Berry     |F  |48973 |\n",
      "|421786|Verhoeff |Xiong     |M  |40000 |\n",
      "|282238|Baumann  |Abdelkader|F  |40000 |\n",
      "+------+---------+----------+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select emp123.emp_no, emp123.last_name, emp123.first_name, \n",
    "emp123.sex,\n",
    "         Salary.salary from emp123 join Salary on emp123.emp_no == Salary.emp_no\"\"\").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. A list showing first name, last name, and hire date for employees who were hired in 1986."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+\n",
      "|first_name|last_name  |hire_date |\n",
      "+----------+-----------+----------+\n",
      "|Xiong     |Verhoeff   |11/26/1987|\n",
      "|Abdelkader|Baumann    |1/18/1991 |\n",
      "|Eran      |Cusworth   |11/14/1986|\n",
      "|Xudong    |Samarati   |11/13/1985|\n",
      "|Lihong    |Magliocco  |10/23/1993|\n",
      "|Shuichi   |Tyugu      |1/17/1995 |\n",
      "|Bojan     |Zallocco   |10/14/1986|\n",
      "|Bilhanan  |Wuwongse   |10/6/1993 |\n",
      "|Elliott   |Perl       |10/29/1987|\n",
      "|Nechama   |Copas      |10/27/1987|\n",
      "|Pantung   |Cools      |1/28/1994 |\n",
      "|Hairong   |Schaar     |12/24/1987|\n",
      "|Mohit     |Speek      |1/14/1986 |\n",
      "|Bader     |Chinal     |1/8/1990  |\n",
      "|Gad       |Nollmann   |10/1/1985 |\n",
      "|Munehiko  |Janocha    |12/11/1988|\n",
      "|Shaowen   |Krone      |11/18/1990|\n",
      "|Rosalie   |Rousseau   |11/7/1998 |\n",
      "|Hitomi    |Gunderson  |11/22/1987|\n",
      "|Leni      |Pusterhofer|1/29/1995 |\n",
      "+----------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select emp123.first_name, emp123.last_name,emp123.hire_date from emp123 \n",
    "          where hire_date BETWEEN '1/1/1986' AND '12/31/1986' \"\"\").show(20,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. A list showing the manager of each department with the following information: department number, department name, the manager's employee number, last name, first name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+------------+-----------+\n",
      "|dept_no|dept_name           |emp_no|last_name   |first_name |\n",
      "+-------+--------------------+------+------------+-----------+\n",
      "|d009   |\"Customer Service\"  |111877|Spinelli    |Xiaobin    |\n",
      "|d008   |\"Research\"          |111534|Kambil      |Hilary     |\n",
      "|d006   |\"Quality Management\"|110765|Hofmeyr     |Rutger     |\n",
      "|d004   |\"Production\"        |110420|Ghazalie    |Oscar      |\n",
      "|d006   |\"Quality Management\"|110725|Onuegbe     |Peternela  |\n",
      "|d001   |\"Marketing\"         |110022|Markovitch  |Margareta  |\n",
      "|d007   |\"Sales\"             |111035|Kaelbling   |Przemyslawa|\n",
      "|d005   |\"development\"       |110511|Hagimont    |DeForest   |\n",
      "|d005   |\"development\"       |110567|DasSarma    |Leon       |\n",
      "|d003   |\"Human Resources\"   |110183|Ossenbruggen|Shirish    |\n",
      "+-------+--------------------+------+------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select depart.dept_no, depart.dept_name, emp123.emp_no,\n",
    "         emp123.last_name,emp123.first_name from depart join\n",
    "         depart_manager on depart.dept_no == depart_manager.dept_no\n",
    "         join emp123 on depart_manager.emp_no == emp123.emp_no\"\"\").show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. A list showing the department of each employee with the following information: employee number, last name, first name, and department name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------+-----------+\n",
      "|dept_name           |emp_no|last_name   |first_name |\n",
      "+--------------------+------+------------+-----------+\n",
      "|\"Customer Service\"  |111877|Spinelli    |Xiaobin    |\n",
      "|\"Research\"          |111534|Kambil      |Hilary     |\n",
      "|\"Quality Management\"|110765|Hofmeyr     |Rutger     |\n",
      "|\"Production\"        |110420|Ghazalie    |Oscar      |\n",
      "|\"Quality Management\"|110725|Onuegbe     |Peternela  |\n",
      "|\"Marketing\"         |110022|Markovitch  |Margareta  |\n",
      "|\"Sales\"             |111035|Kaelbling   |Przemyslawa|\n",
      "|\"development\"       |110511|Hagimont    |DeForest   |\n",
      "|\"development\"       |110567|DasSarma    |Leon       |\n",
      "|\"Human Resources\"   |110183|Ossenbruggen|Shirish    |\n",
      "+--------------------+------+------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select depart.dept_name, emp123.emp_no,\n",
    "         emp123.last_name,emp123.first_name from depart join\n",
    "         depart_manager on depart.dept_no == depart_manager.dept_no\n",
    "         join emp123 on depart_manager.emp_no == emp123.emp_no\"\"\").show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 A list showing first name, last name, and sex for employees whose first name is \"Hercules\" and last names begin with \"B.â€œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---+\n",
      "|first_name|last_name    |sex|\n",
      "+----------+-------------+---+\n",
      "|Hercules  |Baer         |M  |\n",
      "|Hercules  |Biron        |F  |\n",
      "|Hercules  |Birge        |F  |\n",
      "|Hercules  |Berstel      |F  |\n",
      "|Hercules  |Bernatsky    |M  |\n",
      "|Hercules  |Bail         |F  |\n",
      "|Hercules  |Bodoff       |M  |\n",
      "|Hercules  |Benantar     |F  |\n",
      "|Hercules  |Basagni      |M  |\n",
      "|Hercules  |Bernardinello|F  |\n",
      "+----------+-------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select emp123.first_name,emp123.last_name,emp123.sex\n",
    "         from emp123 where first_name = 'Hercules' and last_name like 'B%'\"\"\").show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. A list showing all employees in the Sales department, including their employee number, last name, first name, and department name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+\n",
      "|last_name|first_name|dept_name|\n",
      "+---------+----------+---------+\n",
      "+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select emp123.last_name,emp123.first_name,depart.dept_name from dept_employee\n",
    "    join emp123 on dept_employee.emp_no == emp123.emp_no join depart\n",
    "    on dept_employee.dept_no == depart.dept_no where depart.dept_name = 'Sales'\"\"\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.A list showing all employees in the Sales and Development departments, including their employee number, last name, first name, and department name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+\n",
      "|last_name|first_name|dept_name|\n",
      "+---------+----------+---------+\n",
      "+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select emp123.last_name,emp123.first_name,depart.dept_name from dept_employee\n",
    "    join emp123 on dept_employee.emp_no == emp123.emp_no join depart\n",
    "    on dept_employee.dept_no == depart.dept_no where depart.dept_name = 'Sales'\n",
    "    or depart.dept_name= 'Development'\"\"\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.A list showing the frequency count of employee last names, in descending order. ( i.e., how many employees share each last name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= spark.sql(\"select* from emp123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|       last_name|count|\n",
      "+----------------+-----+\n",
      "|       Delgrossi|  188|\n",
      "|         Rullman|  171|\n",
      "|        Hagimont|  164|\n",
      "|         Lanteri|  173|\n",
      "|          Nanard|  194|\n",
      "|          Saoudi|  176|\n",
      "|   Beutelspacher|  180|\n",
      "|          Butner|  193|\n",
      "|          Stille|  165|\n",
      "|          Aamodt|  205|\n",
      "|Sankaranarayanan|  183|\n",
      "|         Kleiser|  193|\n",
      "|       Besselaar|  188|\n",
      "|         Solovay|  178|\n",
      "|          Ritcey|  156|\n",
      "|      Hinsberger|  187|\n",
      "|            Lund|  170|\n",
      "|     Shokrollahi|  179|\n",
      "|      Taubenfeld|  200|\n",
      "|            Gips|  167|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.groupBy('last_name').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.Histogram to show the salary distribution among the employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = spark.sql(\"select * from emp123 join Salary on emp123.emp_no == Salary.emp_no\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Bar graph to show the Average salary per title (designation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= spark.sql(\"select * from emp123 join Salary on emp123.emp_no ==Salary.emp_no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|sex|sum(salary)|\n",
      "+---+-----------+\n",
      "|  F| 6357161017|\n",
      "|  M| 9535330016|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.groupBy('sex').sum('salary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|left_|sum(salary)|\n",
      "+-----+-----------+\n",
      "|    0|14316882768|\n",
      "|    1| 1575608265|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.groupBy('left_').sum('salary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12:3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "b= spark.sql(\"select * from depart_manager join emp123 on depart_manager.emp_no == emp123.emp_no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|dept_no|count|\n",
      "+-------+-----+\n",
      "|   d005|    2|\n",
      "|   d009|    4|\n",
      "|   d003|    2|\n",
      "|   d001|    2|\n",
      "|   d007|    2|\n",
      "|   d004|    4|\n",
      "|   d002|    2|\n",
      "|   d006|    4|\n",
      "|   d008|    2|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b.groupBy(\"dept_no\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12:4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "c= spark.sql(\"select * from depart join dept_employee on depart.dept_no==dept_employee.dept_no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           dept_name|count|\n",
      "+--------------------+-----+\n",
      "|\"Quality Management\"|20117|\n",
      "|        \"Production\"|73485|\n",
      "|       \"development\"|85707|\n",
      "|          \"Research\"|21126|\n",
      "|             \"Sales\"|52245|\n",
      "|           \"Finance\"|17346|\n",
      "|   \"Human Resources\"|17786|\n",
      "|         \"Marketing\"|20211|\n",
      "|  \"Customer Service\"|23580|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.groupBy('dept_name').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A.Read the data from Hive tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data from hive table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|dept_no|           dept_name|\n",
      "+-------+--------------------+\n",
      "|   d001|         \"Marketing\"|\n",
      "|   d002|           \"Finance\"|\n",
      "|   d003|   \"Human Resources\"|\n",
      "|   d004|        \"Production\"|\n",
      "|   d005|       \"development\"|\n",
      "|   d006|\"Quality Management\"|\n",
      "|   d007|             \"Sales\"|\n",
      "|   d008|          \"Research\"|\n",
      "|   d009|  \"Customer Service\"|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"select * from depart\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|emp_no|salary|\n",
      "+------+------+\n",
      "| 10001| 60117|\n",
      "| 10002| 65828|\n",
      "| 10003| 40006|\n",
      "| 10004| 40054|\n",
      "| 10005| 78228|\n",
      "| 10006| 40000|\n",
      "| 10007| 56724|\n",
      "| 10008| 46671|\n",
      "| 10009| 60929|\n",
      "| 10010| 72488|\n",
      "| 10011| 42365|\n",
      "| 10012| 40000|\n",
      "| 10013| 40000|\n",
      "| 10014| 46168|\n",
      "| 10015| 40000|\n",
      "| 10016| 70889|\n",
      "| 10017| 71380|\n",
      "| 10018| 55881|\n",
      "| 10019| 44276|\n",
      "| 10020| 40000|\n",
      "+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+----------+----------+---+----------+--------------+-----------------------+-----+----------+\n",
      "|emp_no|emp_title_id|birth_date|first_name| last_name|sex| hire_date|no_of_projects|last_performance_rating|left_| last_date|\n",
      "+------+------------+----------+----------+----------+---+----------+--------------+-----------------------+-----+----------+\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "+------+------------+----------+----------+----------+---+----------+--------------+-----------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from emp123\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|dept_no|emp_no|\n",
      "+-------+------+\n",
      "|   d001|110022|\n",
      "|   d001|110039|\n",
      "|   d002|110085|\n",
      "|   d002|110114|\n",
      "|   d003|110183|\n",
      "|   d003|110228|\n",
      "|   d004|110303|\n",
      "|   d004|110344|\n",
      "|   d004|110386|\n",
      "|   d004|110420|\n",
      "|   d005|110511|\n",
      "|   d005|110567|\n",
      "|   d006|110725|\n",
      "|   d006|110765|\n",
      "|   d006|110800|\n",
      "|   d006|110854|\n",
      "|   d007|111035|\n",
      "|   d007|111133|\n",
      "|   d008|111400|\n",
      "|   d008|111534|\n",
      "+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from depart_manager\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|emp_no|dept_no|\n",
      "+------+-------+\n",
      "| 10001|   d005|\n",
      "| 10002|   d007|\n",
      "| 10003|   d004|\n",
      "| 10004|   d004|\n",
      "| 10005|   d003|\n",
      "| 10006|   d005|\n",
      "| 10007|   d008|\n",
      "| 10008|   d005|\n",
      "| 10009|   d006|\n",
      "| 10010|   d004|\n",
      "| 10010|   d006|\n",
      "| 10011|   d009|\n",
      "| 10012|   d005|\n",
      "| 10013|   d003|\n",
      "| 10014|   d005|\n",
      "| 10015|   d008|\n",
      "| 10016|   d007|\n",
      "| 10017|   d001|\n",
      "| 10018|   d004|\n",
      "| 10018|   d005|\n",
      "+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from dept_employee\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|title_id|             title|\n",
      "+--------+------------------+\n",
      "|   s0001|             Staff|\n",
      "|   s0002|      Senior Staff|\n",
      "|   e0001|Assistant Engineer|\n",
      "|   e0002|          Engineer|\n",
      "|   e0003|   Senior Engineer|\n",
      "|   e0004|  Technique Leader|\n",
      "|   m0001|           Manager|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from title\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B.You required to join all the tables at employee level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all the tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= spark.sql(\"\"\"select emp123.emp_no, emp123.emp_title_id, emp123.birth_date, emp123.first_name,\n",
    "    emp123.last_name, emp123.sex, emp123.hire_date, emp123.no_of_projects, emp123.last_performance_rating,\n",
    "    emp123.left_,emp123.last_date, Salary.salary, title.title, dept_employee.dept_no,\n",
    "    depart.dept_name from emp123 join dept_employee on emp123.emp_no == dept_employee.emp_no join title\n",
    "    on emp123.emp_title_id == title.title_id join Salary on emp123.emp_no == Salary.emp_no join depart on \n",
    "    dept_employee.dept_no == depart.dept_no\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: string (nullable = true)\n",
      " |-- emp_title_id: string (nullable = true)\n",
      " |-- birth_date: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      " |-- no_of_projects: integer (nullable = true)\n",
      " |-- last_performance_rating: string (nullable = true)\n",
      " |-- left_: string (nullable = true)\n",
      " |-- last_date: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+----------+---------+---+----------+--------------+-----------------------+-----+---------+------+---------------+-------+--------------------+\n",
      "|emp_no|emp_title_id|birth_date|first_name|last_name|sex| hire_date|no_of_projects|last_performance_rating|left_|last_date|salary|          title|dept_no|           dept_name|\n",
      "+------+------------+----------+----------+---------+---+----------+--------------+-----------------------+-----+---------+------+---------------+-------+--------------------+\n",
      "| 40000|          Staff|   d002|           \"Finance\"|\n",
      "| 53422|       Engineer|   d004|        \"Production\"|\n",
      "| 48973|       Engineer|   d004|        \"Production\"|\n",
      "| 40000|          Staff|   d003|   \"Human Resources\"|\n",
      "| 40000|Senior Engineer|   d006|\"Quality Management\"|\n",
      "+------+------------+----------+----------+---------+---+----------+--------------+-----------------------+-----+---------+------+---------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emp_no',\n",
       " 'emp_title_id',\n",
       " 'birth_date',\n",
       " 'first_name',\n",
       " 'last_name',\n",
       " 'sex',\n",
       " 'hire_date',\n",
       " 'no_of_projects',\n",
       " 'last_performance_rating',\n",
       " 'left_',\n",
       " 'last_date',\n",
       " 'salary',\n",
       " 'title',\n",
       " 'dept_no',\n",
       " 'dept_name']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|    no_of_projects|            salary|\n",
      "+-------+------------------+------------------+\n",
      "|  count|            331603|            331603|\n",
      "|   mean|5.5048567111877755| 52972.22543221865|\n",
      "| stddev|2.8728285038833126|14299.003000805926|\n",
      "|    min|                 1|             40000|\n",
      "|    25%|                 3|             40000|\n",
      "|    50%|                 6|             48688|\n",
      "|    75%|                 8|             61761|\n",
      "|    max|                10|            129492|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating the summary report of Countinous Variable\n",
    "df.select(['no_of_projects','salary']).summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+----------+---------+---+----------+--------------+-----------------------+-----+----------+------+---------------+-------+--------------------+\n",
      "|emp_no|emp_title_id|birth_date|first_name|last_name|sex| hire_date|no_of_projects|last_performance_rating|left_| last_date|salary|          title|dept_no|           dept_name|\n",
      "+------+------------+----------+----------+---------+---+----------+--------------+-----------------------+-----+----------+------+---------------+-------+--------------------+\n",
      "| 40000|          Staff|   d002|           \"Finance\"|\n",
      "| 53422|       Engineer|   d004|        \"Production\"|\n",
      "| 48973|       Engineer|   d004|        \"Production\"|\n",
      "| 40000|          Staff|   d003|   \"Human Resources\"|\n",
      "| 40000|Senior Engineer|   d006|\"Quality Management\"|\n",
      "| 40000|Senior Engineer|   d006|\"Quality Management\"|\n",
      "| 56087|          Staff|   d003|   \"Human Resources\"|\n",
      "| 40000|   Senior Staff|   d002|           \"Finance\"|\n",
      "| 54816|       Engineer|   d006|\"Quality Management\"|\n",
      "| 40000|   Senior Staff|   d009|  \"Customer Service\"|\n",
      "+------+------------+----------+----------+---------+---+----------+--------------+-----------------------+-----+----------+------+---------------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: string (nullable = true)\n",
      " |-- emp_title_id: string (nullable = true)\n",
      " |-- birth_date: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      " |-- no_of_projects: integer (nullable = true)\n",
      " |-- last_performance_rating: string (nullable = true)\n",
      " |-- left_: string (nullable = true)\n",
      " |-- last_date: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding all categorical features\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, PolynomialExpansion, VectorIndexer,OneHotEncoderEstimator,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "continious_features =['emp_no','emp_title_id','birth_date','first_name','last_name','hire_date',\n",
    "                      'no_of_projects','last_date','title','dept_no']\n",
    "categorical_features= ['sex','last_performance_rating','salary','dept_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----------------------+-----------------------------+------+------------+--------------------+---------------+\n",
      "|sex|sex_index|last_performance_rating|last_performance_rating_index|salary|salary_index|           dept_name|dept_name_index|\n",
      "+---+---------+-----------------------+-----------------------------+------+------------+--------------------+---------------+\n",
      "|  M|      0.0|                      A|                          1.0| 40000|         0.0|           \"Finance\"|            8.0|\n",
      "|  F|      1.0|                      C|                          2.0| 53422|      4384.0|        \"Production\"|            1.0|\n",
      "|  F|      1.0|                      A|                          1.0| 48973|     20860.0|        \"Production\"|            1.0|\n",
      "|  M|      0.0|                      C|                          2.0| 40000|         0.0|   \"Human Resources\"|            7.0|\n",
      "|  F|      1.0|                      B|                          0.0| 40000|         0.0|\"Quality Management\"|            6.0|\n",
      "|  M|      0.0|                      B|                          0.0| 40000|         0.0|\"Quality Management\"|            6.0|\n",
      "|  M|      0.0|                      A|                          1.0| 56087|     10967.0|   \"Human Resources\"|            7.0|\n",
      "|  M|      0.0|                      A|                          1.0| 40000|         0.0|           \"Finance\"|            8.0|\n",
      "|  M|      0.0|                      A|                          1.0| 54816|     22643.0|\"Quality Management\"|            6.0|\n",
      "|  F|      1.0|                      B|                          0.0| 40000|         0.0|  \"Customer Service\"|            3.0|\n",
      "+---+---------+-----------------------+-----------------------------+------+------------+--------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SI_sex = StringIndexer(inputCol='sex', outputCol='sex_index')\n",
    "SI_last_performance_rating= StringIndexer(inputCol='last_performance_rating', outputCol='last_performance_rating_index')\n",
    "SI_salary=StringIndexer(inputCol='salary', outputCol='salary_index')\n",
    "SI_dept_name=StringIndexer(inputCol='dept_name', outputCol='dept_name_index')\n",
    "\n",
    "df_new=SI_sex.fit(df_new).transform(df_new)\n",
    "df_new=SI_last_performance_rating.fit(df_new).transform(df_new)\n",
    "df_new=SI_salary.fit(df_new).transform(df_new)\n",
    "df_new=SI_dept_name.fit(df_new).transform(df_new)\n",
    "\n",
    "df_new.select('sex','sex_index','last_performance_rating','last_performance_rating_index','salary','salary_index','dept_name','dept_name_index').show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create object and specify input and output column\n",
    "encoder= OneHotEncoderEstimator().setInputCols(['sex_index','last_performance_rating_index','salary_index','dept_name_index']).setOutputCols(['sex_vec','last_performance_rating_vec','salary_vec','dept_name_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`sex_vec`' given input columns: [sex_index, default.emp123.sex, default.emp123.left_, last_performance_rating_index, dept_name_index, default.emp123.birth_date, default.salary.salary, salary_index, default.depart.dept_name, default.dept_employee.dept_no, default.emp123.last_name, default.emp123.no_of_projects, default.emp123.emp_no, default.emp123.first_name, default.title.title, default.emp123.emp_title_id, default.emp123.hire_date, default.emp123.last_performance_rating, default.emp123.last_date];;\\n'Project ['sex_vec, 'last_performance_rating_vec, 'salary_vec, 'dept_name_vec]\\n+- Project [emp_no#544, emp_title_id#545, birth_date#546, first_name#547, last_name#548, sex#549, hire_date#550, no_of_projects#551, last_performance_rating#552, left_#553, last_date#554, salary#560L, title#558, dept_no#556, dept_name#562, sex_index#858, last_performance_rating_index#894, salary_index#932, UDF(cast(dept_name#562 as string)) AS dept_name_index#972]\\n   +- Project [emp_no#544, emp_title_id#545, birth_date#546, first_name#547, last_name#548, sex#549, hire_date#550, no_of_projects#551, last_performance_rating#552, left_#553, last_date#554, salary#560L, title#558, dept_no#556, dept_name#562, sex_index#858, last_performance_rating_index#894, UDF(cast(salary#560L as string)) AS salary_index#932]\\n      +- Project [emp_no#544, emp_title_id#545, birth_date#546, first_name#547, last_name#548, sex#549, hire_date#550, no_of_projects#551, last_performance_rating#552, left_#553, last_date#554, salary#560L, title#558, dept_no#556, dept_name#562, sex_index#858, UDF(cast(last_performance_rating#552 as string)) AS last_performance_rating_index#894]\\n         +- Project [emp_no#544, emp_title_id#545, birth_date#546, first_name#547, last_name#548, sex#549, hire_date#550, no_of_projects#551, last_performance_rating#552, left_#553, last_date#554, salary#560L, title#558, dept_no#556, dept_name#562, UDF(cast(sex#549 as string)) AS sex_index#858]\\n            +- Project [emp_no#544, emp_title_id#545, birth_date#546, first_name#547, last_name#548, sex#549, hire_date#550, no_of_projects#551, last_performance_rating#552, left_#553, last_date#554, salary#560L, title#558, dept_no#556, dept_name#562]\\n               +- Join Inner, (dept_no#556 = dept_no#561)\\n                  :- Join Inner, (cast(emp_no#544 as bigint) = emp_no#559L)\\n                  :  :- Join Inner, (emp_title_id#545 = title_id#557)\\n                  :  :  :- Join Inner, (cast(emp_no#544 as int) = emp_no#555)\\n                  :  :  :  :- SubqueryAlias `default`.`emp123`\\n                  :  :  :  :  +- HiveTableRelation `default`.`emp123`, org.apache.hadoop.hive.serde2.avro.AvroSerDe, [emp_no#544, emp_title_id#545, birth_date#546, first_name#547, last_name#548, sex#549, hire_date#550, no_of_projects#551, last_performance_rating#552, left_#553, last_date#554]\\n                  :  :  :  +- SubqueryAlias `default`.`dept_employee`\\n                  :  :  :     +- HiveTableRelation `default`.`dept_employee`, org.apache.hadoop.hive.serde2.avro.AvroSerDe, [emp_no#555, dept_no#556]\\n                  :  :  +- SubqueryAlias `default`.`title`\\n                  :  :     +- HiveTableRelation `default`.`title`, org.apache.hadoop.hive.serde2.avro.AvroSerDe, [title_id#557, title#558]\\n                  :  +- SubqueryAlias `default`.`salary`\\n                  :     +- HiveTableRelation `default`.`salary`, org.apache.hadoop.hive.serde2.avro.AvroSerDe, [emp_no#559L, salary#560L]\\n                  +- SubqueryAlias `default`.`depart`\\n                     +- HiveTableRelation `default`.`depart`, org.apache.hadoop.hive.serde2.avro.AvroSerDe, [dept_no#561, dept_name#562]\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o256.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`sex_vec`' given input columns: [sex_index, default.emp123.sex, default.emp123.left_, last_performance_rating_index, dept_name_index, default.emp123.birth_date, default.salary.salary, salary_index, default.depart.dept_name, default.dept_employee.dept_no, default.emp123.last_name, default.emp123.no_of_projects, default.emp123.emp_no, default.emp123.first_name, default.title.title, default.emp123.emp_title_id, default.emp123.hire_date, default.emp123.last_performance_rating, default.emp123.last_date];;\n'Project ['sex_vec, 'last_performance_rating_vec, 'salary_vec, 'dept_name_vec]\n+- Project [emp_no#544, emp_title_id#545, birth_date#546, first_name#547, last_name#548, sex#549, hire_date#550, no_of_projects#551, last_performance_rating#552, left_#553, last_date#554, salary#560L, title#558, dept_no#556, dept_name#562, sex_index#858, last_performance_rating_index#894, salary_index#932, UDF(cast(dept_name#562 as string)) AS dept_name_index#972]\n   +- Project [emp_no#544, emp_title_id#545, birth_date#546, first_name#547, last_name#548, sex#549, hire_date#550, no_of_projects#551, last_performance_rating#552, left_#553, last_date#554, salary#560L, title#558, dept_no#556, dept_name#562, sex_index#858, last_performance_rating_index#894, UDF(cast(salary#560L as string)) AS salary_index#932]\n      +- Project [emp_no#544, emp_title_id#545, birth_date#546, first_name#547, last_name#548, sex#549, hire_date#550, no_of_projects#551, last_performance_rating#552, left_#553, last_date#554, salary#560L, title#558, dept_no#556, dept_name#562, sex_index#858, UDF(cast(last_performance_rating#552 as string)) AS last_performance_rating_index#894]\n         +- Project [emp_no#544, emp_title_id#545, birth_date#546, first_name#547, last_name#548, sex#549, hire_date#550, no_of_projects#551, last_performance_rating#552, left_#553, last_date#554, salary#560L, title#558, dept_no#556, dept_name#562, UDF(cast(sex#549 as string)) AS sex_index#858]\n            +- Project [emp_no#544, emp_title_id#545, birth_date#546, first_name#547, last_name#548, sex#549, hire_date#550, no_of_projects#551, last_performance_rating#552, left_#553, last_date#554, salary#560L, title#558, dept_no#556, dept_name#562]\n               +- Join Inner, (dept_no#556 = dept_no#561)\n                  :- Join Inner, (cast(emp_no#544 as bigint) = emp_no#559L)\n                  :  :- Join Inner, (emp_title_id#545 = title_id#557)\n                  :  :  :- Join Inner, (cast(emp_no#544 as int) = emp_no#555)\n                  :  :  :  :- SubqueryAlias `default`.`emp123`\n                  :  :  :  :  +- HiveTableRelation `default`.`emp123`, org.apache.hadoop.hive.serde2.avro.AvroSerDe, [emp_no#544, emp_title_id#545, birth_date#546, first_name#547, last_name#548, sex#549, hire_date#550, no_of_projects#551, last_performance_rating#552, left_#553, last_date#554]\n                  :  :  :  +- SubqueryAlias `default`.`dept_employee`\n                  :  :  :     +- HiveTableRelation `default`.`dept_employee`, org.apache.hadoop.hive.serde2.avro.AvroSerDe, [emp_no#555, dept_no#556]\n                  :  :  +- SubqueryAlias `default`.`title`\n                  :  :     +- HiveTableRelation `default`.`title`, org.apache.hadoop.hive.serde2.avro.AvroSerDe, [title_id#557, title#558]\n                  :  +- SubqueryAlias `default`.`salary`\n                  :     +- HiveTableRelation `default`.`salary`, org.apache.hadoop.hive.serde2.avro.AvroSerDe, [emp_no#559L, salary#560L]\n                  +- SubqueryAlias `default`.`depart`\n                     +- HiveTableRelation `default`.`depart`, org.apache.hadoop.hive.serde2.avro.AvroSerDe, [dept_no#561, dept_name#562]\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3407)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1335)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-5378d63daa6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sex_vec'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'last_performance_rating_vec'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'salary_vec'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dept_name_vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \"\"\"\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`sex_vec`' given input columns: [sex_index, default.emp123.sex, default.emp123.left_, last_performance_rating_index, dept_name_index, default.emp123.birth_date, default.salary.salary, salary_index, default.depart.dept_name, default.dept_employee.dept_no, default.emp123.last_name, default.emp123.no_of_projects, default.emp123.emp_no, default.emp123.first_name, default.title.title, default.emp123.emp_title_id, default.emp123.hire_date, default.emp123.last_performance_rating, default.emp123.last_date];;\\n'Project ['sex_vec, 'last_performance_rating_vec, 'salary_vec, 'dept_name_vec]\\n+- Project [emp_no#544, emp_title_id#545, birth_date#546, first_name#547, last_name#548, sex#549, hire_date#550, no_of_projects#551, last_performance_rating#552, left_#553, last_date#554, salary#560L, title#558, dept_no#556, dept_name#562, sex_index#858, last_performance_rating_index#894, salary_index#932, UDF(cast(dept_name#562 as string)) AS dept_name_index#972]\\n   +- Project [emp_no#544, emp_title_id#545, birth_date#546, first_name#547, last_name#548, sex#549, hire_date#550, no_of_projects#551, last_performance_rating#552, left_#553, last_date#554, salary#560L, title#558, dept_no#556, dept_name#562, sex_index#858, last_performance_rating_index#894, UDF(cast(salary#560L as string)) AS salary_index#932]\\n      +- Project [emp_no#544, emp_title_id#545, birth_date#546, first_name#547, last_name#548, sex#549, hire_date#550, no_of_projects#551, last_performance_rating#552, left_#553, last_date#554, salary#560L, title#558, dept_no#556, dept_name#562, sex_index#858, UDF(cast(last_performance_rating#552 as string)) AS last_performance_rating_index#894]\\n         +- Project [emp_no#544, emp_title_id#545, birth_date#546, first_name#547, last_name#548, sex#549, hire_date#550, no_of_projects#551, last_performance_rating#552, left_#553, last_date#554, salary#560L, title#558, dept_no#556, dept_name#562, UDF(cast(sex#549 as string)) AS sex_index#858]\\n            +- Project [emp_no#544, emp_title_id#545, birth_date#546, first_name#547, last_name#548, sex#549, hire_date#550, no_of_projects#551, last_performance_rating#552, left_#553, last_date#554, salary#560L, title#558, dept_no#556, dept_name#562]\\n               +- Join Inner, (dept_no#556 = dept_no#561)\\n                  :- Join Inner, (cast(emp_no#544 as bigint) = emp_no#559L)\\n                  :  :- Join Inner, (emp_title_id#545 = title_id#557)\\n                  :  :  :- Join Inner, (cast(emp_no#544 as int) = emp_no#555)\\n                  :  :  :  :- SubqueryAlias `default`.`emp123`\\n                  :  :  :  :  +- HiveTableRelation `default`.`emp123`, org.apache.hadoop.hive.serde2.avro.AvroSerDe, [emp_no#544, emp_title_id#545, birth_date#546, first_name#547, last_name#548, sex#549, hire_date#550, no_of_projects#551, last_performance_rating#552, left_#553, last_date#554]\\n                  :  :  :  +- SubqueryAlias `default`.`dept_employee`\\n                  :  :  :     +- HiveTableRelation `default`.`dept_employee`, org.apache.hadoop.hive.serde2.avro.AvroSerDe, [emp_no#555, dept_no#556]\\n                  :  :  +- SubqueryAlias `default`.`title`\\n                  :  :     +- HiveTableRelation `default`.`title`, org.apache.hadoop.hive.serde2.avro.AvroSerDe, [title_id#557, title#558]\\n                  :  +- SubqueryAlias `default`.`salary`\\n                  :     +- HiveTableRelation `default`.`salary`, org.apache.hadoop.hive.serde2.avro.AvroSerDe, [emp_no#559L, salary#560L]\\n                  +- SubqueryAlias `default`.`depart`\\n                     +- HiveTableRelation `default`.`depart`, org.apache.hadoop.hive.serde2.avro.AvroSerDe, [dept_no#561, dept_name#562]\\n\""
     ]
    }
   ],
   "source": [
    "df_new.select('sex_vec','last_performance_rating_vec','salary_vec','dept_name_vec').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create vector from categorical columns\n",
    "featureCols =['sex_vec','last_performance_rating_vec','salary_vec','dept_name_vec']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vector_assembler = VectorAssembler(inputCols = featureCols,outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_df= Vector_assembler.transform(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_df= caps_df.withColumn('label', when(caps_df.left_ == '1',1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_df.select('left_','label').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRain & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df = caps_df.randomSplit([0.7,0.3], seed= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the Linear Regression Model\n",
    "from pyspark.ml.classification import LogisticRegression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logrg = LogisticRegression(featuresCol= 'features',labelCol='label',maxIter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = logrg.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = lr.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = lr.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train.select(['label',\n",
    " 'rawPrediction',\n",
    " 'probability',\n",
    " 'prediction']).toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test.filter(y_pred_test.label == y_pred_test.prediction).count() / float(y_pred_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
